{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AISentinel - Sentiment Analysis Model Evaluation\n",
    "\n",
    "This notebook demonstrates the complete pipeline for training and evaluating a custom TensorFlow sentiment analysis model for AI tool reviews.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Data Preparation**: Load and preprocess training data\n",
    "2. **Model Architecture**: Build custom LSTM/Transformer models\n",
    "3. **Training**: Train with proper validation and callbacks\n",
    "4. **Evaluation**: Comprehensive metrics and visualizations\n",
    "5. **Testing**: Real-world AI tool review analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "Load and explore the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_dir = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "# Check if data exists, if not prepare it\n",
    "if not (data_dir / \"train.csv\").exists():\n",
    "    print(\"Preparing training data...\")\n",
    "    from src.data_collection.prepare_training_data import prepare_training_data\n",
    "    train_df, val_df, test_df = prepare_training_data(output_dir=data_dir)\n",
    "else:\n",
    "    print(\"Loading existing data...\")\n",
    "    train_df = pd.read_csv(data_dir / \"train.csv\")\n",
    "    val_df = pd.read_csv(data_dir / \"val.csv\")\n",
    "    test_df = pd.read_csv(data_dir / \"test.csv\")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_df):,}\")\n",
    "print(f\"  Val:   {len(val_df):,}\")\n",
    "print(f\"  Test:  {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Sentiment distribution\n",
    "train_df['sentiment'].value_counts().plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Sentiment Distribution (Train)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Sentiment')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Source distribution\n",
    "train_df['source'].value_counts().plot(kind='bar', ax=axes[1], color='lightcoral')\n",
    "axes[1].set_title('Data Source Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Source')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Text length distribution\n",
    "train_df['text_length'] = train_df['text'].str.len()\n",
    "train_df['text_length'].hist(bins=50, ax=axes[2], color='lightgreen', edgecolor='black')\n",
    "axes[2].set_title('Text Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Character Count')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nText length statistics:\")\n",
    "print(train_df['text_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample reviews\n",
    "print(\"Sample reviews from each sentiment class:\\n\")\n",
    "for sentiment in ['positive', 'neutral', 'negative']:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{sentiment.upper()} Examples\")\n",
    "    print('='*60)\n",
    "    samples = train_df[train_df['sentiment'] == sentiment].sample(3)\n",
    "    for i, (_, row) in enumerate(samples.iterrows(), 1):\n",
    "        print(f\"\\n{i}. {row['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture\n",
    "\n",
    "Build custom TensorFlow models for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.model import build_sentiment_model, build_transformer_based_model\n",
    "\n",
    "# Build LSTM model\n",
    "lstm_model = build_sentiment_model(\n",
    "    vocab_size=10000,\n",
    "    embedding_dim=128,\n",
    "    max_length=128,\n",
    "    num_classes=3,\n",
    "    lstm_units=64,\n",
    "    dropout_rate=0.5,\n",
    "    use_attention=True,\n",
    ")\n",
    "\n",
    "print(\"\\nLSTM Model with Attention Mechanism\")\n",
    "print(\"=\"*60)\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model architecture\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(\n",
    "    lstm_model,\n",
    "    to_file='model_architecture.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',\n",
    "    dpi=150\n",
    ")\n",
    "\n",
    "from IPython.display import Image\n",
    "Image('model_architecture.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "Train the model with proper data preprocessing and callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.train_model import SentimentModelTrainer\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize trainer\n",
    "output_dir = PROJECT_ROOT / \"models\" / f\"notebook_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "trainer = SentimentModelTrainer(\n",
    "    data_dir=data_dir,\n",
    "    output_dir=output_dir,\n",
    "    max_vocab_size=10000,\n",
    "    max_length=128,\n",
    "    model_type=\"lstm\",\n",
    ")\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "print(\"Preparing tokenizer and sequences...\")\n",
    "trainer.load_data()\n",
    "tokenizer = trainer.prepare_tokenizer(train_df)\n",
    "\n",
    "X_train, y_train = trainer.prepare_sequences(train_df)\n",
    "X_val, y_val = trainer.prepare_sequences(val_df)\n",
    "X_test, y_test = trainer.prepare_sequences(test_df)\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_val:   {X_val.shape}\")\n",
    "print(f\"  X_test:  {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train model\n",
    "model = trainer.build_model()\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "history = trainer.train(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    epochs=10,  # Reduced for notebook demo\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Analysis\n",
    "\n",
    "Visualize training progress and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "axes[0, 0].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[0, 1].plot(history.history['loss'], label='Train', linewidth=2)\n",
    "axes[0, 1].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision\n",
    "if 'precision' in history.history:\n",
    "    axes[1, 0].plot(history.history['precision'], label='Train', linewidth=2)\n",
    "    axes[1, 0].plot(history.history['val_precision'], label='Validation', linewidth=2)\n",
    "    axes[1, 0].set_title('Model Precision', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Precision')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "if 'recall' in history.history:\n",
    "    axes[1, 1].plot(history.history['recall'], label='Train', linewidth=2)\n",
    "    axes[1, 1].plot(history.history['val_recall'], label='Validation', linewidth=2)\n",
    "    axes[1, 1].set_title('Model Recall', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Recall')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'training_history_detailed.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "Comprehensive evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "metrics = trainer.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {metrics['test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "    yticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "    cbar_kws={'label': 'Count'},\n",
    ")\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'confusion_matrix_detailed.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "print(\"\\nPer-class accuracy:\")\n",
    "for i, label in enumerate(['Negative', 'Neutral', 'Positive']):\n",
    "    class_acc = cm[i, i] / cm[i, :].sum()\n",
    "    print(f\"  {label}: {class_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=['Negative', 'Neutral', 'Positive'],\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for nice display\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction confidence distribution\n",
    "y_pred_probs = model.predict(X_test)\n",
    "confidence_scores = np.max(y_pred_probs, axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Overall confidence\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(confidence_scores, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Confidence Score', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "plt.axvline(confidence_scores.mean(), color='red', linestyle='--', label=f'Mean: {confidence_scores.mean():.3f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Confidence by correctness\n",
    "plt.subplot(1, 2, 2)\n",
    "correct_mask = y_pred == y_test\n",
    "plt.hist(confidence_scores[correct_mask], bins=30, alpha=0.6, label='Correct', edgecolor='black')\n",
    "plt.hist(confidence_scores[~correct_mask], bins=30, alpha=0.6, label='Incorrect', edgecolor='black')\n",
    "plt.xlabel('Confidence Score', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Confidence: Correct vs Incorrect', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMean confidence (correct predictions): {confidence_scores[correct_mask].mean():.3f}\")\n",
    "print(f\"Mean confidence (incorrect predictions): {confidence_scores[~correct_mask].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-World Testing\n",
    "\n",
    "Test the model on real AI tool reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model first\n",
    "trainer.save_model()\n",
    "print(f\"Model saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with real AI tool reviews\n",
    "from src.sentiment_analysis.analyzer import AdvancedSentimentAnalyzer\n",
    "\n",
    "analyzer = AdvancedSentimentAnalyzer(\n",
    "    use_custom_model=True,\n",
    "    custom_model_path=output_dir / \"sentiment_model.keras\",\n",
    "    custom_tokenizer_path=output_dir / \"tokenizer.pkl\",\n",
    ")\n",
    "\n",
    "# Sample reviews\n",
    "test_reviews = [\n",
    "    \"ChatGPT is absolutely amazing! It helps me code so much faster.\",\n",
    "    \"Claude is terrible, keeps giving me wrong answers.\",\n",
    "    \"GitHub Copilot is okay, works for basic stuff.\",\n",
    "    \"Midjourney creates stunning images! Best AI art tool ever.\",\n",
    "    \"Concerned about privacy with DeepSeek. Where is my data stored?\",\n",
    "]\n",
    "\n",
    "print(\"\\nTesting on real AI tool reviews:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "for review in test_reviews:\n",
    "    result = analyzer.analyze(review)\n",
    "    results.append({\n",
    "        'text': review,\n",
    "        'sentiment': result.label,\n",
    "        'confidence': result.confidence,\n",
    "        'score': result.score,\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nReview: {review}\")\n",
    "    print(f\"â†’ Sentiment: {result.label.upper()} (confidence: {result.confidence:.3f})\")\n",
    "\n",
    "# Display as DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nResults Summary:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = {'positive': 'green', 'neutral': 'gray', 'negative': 'red'}\n",
    "bar_colors = [colors[s] for s in results_df['sentiment']]\n",
    "\n",
    "bars = ax.barh(range(len(results_df)), results_df['confidence'], color=bar_colors, alpha=0.7)\n",
    "ax.set_yticks(range(len(results_df)))\n",
    "ax.set_yticklabels([f\"Review {i+1}\" for i in range(len(results_df))])\n",
    "ax.set_xlabel('Confidence Score', fontsize=12)\n",
    "ax.set_title('Sentiment Analysis Results - AI Tool Reviews', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add sentiment labels\n",
    "for i, (idx, row) in enumerate(results_df.iterrows()):\n",
    "    ax.text(row['confidence'] + 0.02, i, row['sentiment'].capitalize(), \n",
    "            va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Summary\n",
    "\n",
    "Final model statistics and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nModel Architecture: LSTM with Attention\")\n",
    "print(f\"Training Samples: {len(train_df):,}\")\n",
    "print(f\"Validation Samples: {len(val_df):,}\")\n",
    "print(f\"Test Samples: {len(test_df):,}\")\n",
    "\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "print(f\"  Test Accuracy: {metrics['test_accuracy']:.2%}\")\n",
    "print(f\"  Vocabulary Size: {len(tokenizer.word_index):,}\")\n",
    "print(f\"  Max Sequence Length: {trainer.max_length}\")\n",
    "\n",
    "print(f\"\\nModel saved to:\")\n",
    "print(f\"  {output_dir}\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(\"  1. Deploy model to production\")\n",
    "print(\"  2. Integrate with data collection pipeline\")\n",
    "print(\"  3. Update dashboard to use custom model\")\n",
    "print(\"  4. Monitor performance on real data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
